# -*- coding: utf-8 -*-
"""mental health helping buddy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mp_sGIsE9GPVgd-oHF8nEFb4tqLydYp2
"""

pip install langchain_groq

from langchain_groq import ChatGroq
llm=ChatGroq(
    temperature= 0,
    groq_api_key='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',
    model= "llama-3.3-70b-versatile"

)


result= llm.invoke("What is Bipolar Disorder?")
print(result.content)

!pip install Pypdf

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import retrieval_qa
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

!pip install -U langchain-community

import os
from langchain.chains import RetrievalQA

!pip install chromadb

import gradio as gr
import time
import os
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

# === LLM + VectorDB Setup ===
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key='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',
        model="llama-3.3-70b-versatile"
    )
    return llm

def create_vector_db():
    loader = DirectoryLoader("/content/data", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name="BAAI/bge-large-en-v1.5")
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory="./Chroma_db")
    vector_db.persist()
    print("Chroma db created and saved successfully")
    return vector_db

create_vector_db()

import gradio as gr
import time
import os
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

# === LLM + VectorDB Setup ===
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key='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',
        model="llama-3.3-70b-versatile"
    )
    return llm

def create_vector_db():
    loader = DirectoryLoader("/content/data", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name="BAAI/bge-large-en-v1.5")
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory="./Chroma_db")
    vector_db.persist()
    print("Chroma db created and saved successfully")
    return vector_db

def setup_q_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a mental health chatbot. Respond through this:
    {context}
    User: {question}
    Chatbot: """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=False,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# === Initialize System ===
print("Initializing Chatbot.........")
llm = initialize_llm()
db_path = "/content/Chroma_db"

if not os.path.exists(db_path):
    print("Creating Vector Database.........")
    vector_db = create_vector_db()
else:
    embeddings = HuggingFaceBgeEmbeddings(model_name="BAAI/bge-large-en-v1.5")
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

qa_chain = setup_q_chain(vector_db, llm)

# === Chat logic ===
def add_message(history, message):
    for f in message["files"]:
        history.append({"role": "user", "content": {"path": f}})
    if message["text"]:
        history.append({"role": "user", "content": message["text"]})
    return history, gr.MultimodalTextbox(value=None, interactive=False)

def bot(history):
    user_msg = history[-1]["content"]
    doctor_emoji = "üßë‚Äç‚öïÔ∏è"

    if isinstance(user_msg, dict):
        reply = f"{doctor_emoji} I received your file, but I can't process it yet. Please ask me something."
    else:
        greeting_triggers = ["hi", "hello", "hey", "hii", "heyy", "good morning", "good evening", "good afternoon"]
        lowered_msg = user_msg.lower().strip()
        if any(greet in lowered_msg for greet in greeting_triggers):
            reply = f"{doctor_emoji} Hi there! I am **Buddy**, your friendly mental wellness companion. How can I help you today?"
        else:
            bot_reply = qa_chain.run(user_msg)
            reply = f"{doctor_emoji} {bot_reply}"

    history.append({"role": "assistant", "content": ""})
    for char in reply:
        history[-1]["content"] += char
        time.sleep(0.02)
        yield history



# === Custom Calm UI Theme (CSS) ===
calm_css = """
.gradio-container {
    max-width: 800px;
    margin: auto;
    font-family: 'Segoe UI', sans-serif;
    background: #f0fdf4;
    padding: 20px;
    border-radius: 16px;
    box-shadow: 0 0 12px rgba(0, 100, 80, 0.1);
}

#chatbot {
    border: 1px solid #cce3d6;
    border-radius: 16px;
    background-color: #ffffff;
    padding: 10px;
}

#chatbot .message.user {
    background-color: #e1f5ec;
    border-radius: 12px;
    padding: 8px 12px;
    color: #14532d;
}

#chatbot .message.assistant {
    background-color: #e0f2fe;
    border-radius: 12px;
    padding: 8px 12px;
    color: #0c4a6e;
}

h1 {
    color: #14532d;
    text-align: center;
    font-size: 2.2em;
    margin-bottom: 8px;
}

.gr-markdown {
    color: #334d4d;
    text-align: center;
    font-size: 1.1em;
    margin-bottom: 20px;
}
"""

# === UI ===
with gr.Blocks(css=calm_css) as app:
    gr.Markdown("<h1>üß† Mental Health Chatbot</h1>")
    gr.Markdown("Hi there! I'm here to help you with mental well-being. Ask me anything or upload a file you'd like me to see. üåø")

    chatbot = gr.Chatbot(elem_id="chatbot", bubble_full_width=False, type="messages")

    chat_input = gr.MultimodalTextbox(
        interactive=True,
        file_count="multiple",
        placeholder="Type your message or upload a file...",
        show_label=False,
        sources=["upload"],  # Microphone removed
    )

    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])
    bot_msg = chat_msg.then(bot, chatbot, chatbot)
    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])

app.launch(server_name="0.0.0.0", server_port=8080,share=True, debug=True)







!pip install gradio







